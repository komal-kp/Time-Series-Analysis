# -*- coding: utf-8 -*-
"""mini proj cs575.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dea_UNIcNlVXzcctPO--c1Lx_Msx9i1a

## **Loading necessary libraries**
"""

!pip install statsmodels==0.12.2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(style="whitegrid")
import warnings
from pandas.plotting import autocorrelation_plot
from matplotlib import pyplot
from pandas.plotting import lag_plot
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from sklearn.metrics import r2_score
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error,mean_absolute_error
import statsmodels.api as sm
from math import sqrt
from statsmodels.tsa.seasonal import seasonal_decompose
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from keras.callbacks import EarlyStopping

from keras.layers import Dense, SimpleRNN

"""## **Dataset and its information**


*   Dataset used - The SUNSPOTS dataset from statsmodels' datasets
  


"""

#loading the dataset
data = sm.datasets.sunspots.load_pandas()

#viewing the dataset
data.data

type(data.data)

data.data['YEAR']=pd.to_numeric(data.data['YEAR'].values, downcast='integer')

data = pd.DataFrame(data.data)
data

data['SUNACTIVITY'].describe(include = 'all')

data.isna().any()

#plotting the data(lineplot)

plt.figure(figsize=(12,8))
plt.xticks(np.arange(min(data['YEAR']), max(data['YEAR'])+1, 10),rotation=60)
plt.xlabel('Years')
plt.ylabel('Sunactivity')
plt.plot(data['YEAR'],data['SUNACTIVITY'],'go-',linewidth=1)


plt.title('Sunspots', fontsize=20)

#scatterplot
plt.figure(figsize=(12,8))
plt.xticks(np.arange(min(data['YEAR']), max(data['YEAR'])+1, 20),rotation=60)
plt.xlabel('Years')
plt.ylabel('Sunactivity')
colors = np.random.rand(309)

plt.scatter(data.YEAR,data.SUNACTIVITY,c=colors,alpha=0.6)


plt.title('Sunspots', fontsize=20)

"""# **Stationarity tests**

1. **Data Visualisation**
"""

plt.figure(figsize=(12,8))
plt.xticks(np.arange(min(data['YEAR']), max(data['YEAR'])+1, 20),rotation=60)
plt.xlabel('Years')
plt.ylabel('Sunactivity')
colors = np.random.rand(309)

plt.hist((data['SUNACTIVITY']),color= 'pink',bins=100)

plt.title('Sunspots', fontsize=20)

"""It can be concluded from the figure that data is unevenly distributed, making it non stationary.

2. **Checking Mean**
"""

X = data.SUNACTIVITY.values
split = int(len(X)/3 )
X1,X2 = X[0:split],X[split*2:split*3]
mean1, mean2 = X1.mean(), X2.mean()
var1, var2 = X1.var(), X2.var()
print('mean1=%f, mean2=%f' % (mean1, mean2))
print('variance1=%f, variance2=%f' % (var1, var2))

"""The dataset is split into 3 parts and the means of 1st and 3rd part is compared. It's clear from the differnce in both means(mean1 and mean2) that data is non-stationary.

3. **ADF and KPSS tests**
"""

from statsmodels.tsa.stattools import adfuller
result = adfuller(data['SUNACTIVITY'].values)
print('ADF Statistics: ',result[0])
print('p-value: ', result[1])
print('Critical Values:')
for key, value in result[4].items():
    print(key, value)

if result[0] < result[4]["5%"]:
    print ("Time Series is Stationary")
   
else:
    print ("Time Series is Non-Stationary")

from statsmodels.tsa.stattools import kpss
result = kpss(data['SUNACTIVITY'].values,lags='auto')
print('KPSS Statistics: ',result[0])
print('p-value: ', result[1])
print('Critical Values:')
for key, value in result[3].items():
    print(key, value)

if result[0] > result[3]["5%"]:
    print ("Time Series is not Stationary")
else:
    print ("Time Series is Stationary")

"""Both ADF and KPSS tests show that the data is non stationary.

***Making the data stationary using differencing***
"""

data['sun_diff'] = data['SUNACTIVITY'].diff(periods=1)

diff_vals = data['sun_diff'][1:]
diff_vals= pd.DataFrame(diff_vals,columns=['sun_diff'])
diff_vals

#plotting the differenced series 
plt.figure(figsize=(12,8))
plt.xticks(np.arange(min(data['YEAR']), max(data['YEAR'])+1, 20),rotation=60)
plt.xlabel('Years')
plt.ylabel('Sunactivity')
plt.plot(data['YEAR'][1:],diff_vals.sun_diff,'y^-',linewidth=2,label='Differenced')
plt.plot(data['YEAR'],data['SUNACTIVITY'],'go-',linewidth=0.4,label='Original')
plt.legend(loc='upper left')
plt.title('Sunspots', fontsize=20)

"""***Verifying that data is now stationary after differncing***

1. **Visualization**
"""

plt.figure(figsize=(12,8))
plt.xticks(np.arange(min(data['YEAR']), max(data['YEAR'])+1, 20),rotation=60)
plt.xlabel('Years')
plt.ylabel('Sunactivity')
colors = np.random.rand(309)

plt.hist((diff_vals.sun_diff),color= 'pink',bins=80)

plt.title('Sunspots', fontsize=20)

"""As the curve is bell shaped, indicating the gaussian distribution of the differnced series, it can be concluded that it's stationary now.

2. **Checking Mean**
"""

X = diff_vals.sun_diff.values
split = int(len(X)/3 )
X1,X2 = X[0:split],X[split*2:split*3]
mean1, mean2 = X1.mean(), X2.mean()
var1, var2 = X1.var(), X2.var()
print('mean1=%f, mean2=%f' % (mean1, mean2))
print('variance1=%f, variance2=%f' % (var1, var2))

"""As it's seen from the above figures that both means(mean1 and mean2) are much similar and close to zero, this data can be called stationary.

3. **ADF and KPSS tests**
"""

result = adfuller(diff_vals['sun_diff'].values)
print('ADF Statistics: ',result[0])
print('p-value: ', result[1])
print('Critical Values:')
for key, value in result[4].items():
    print(key, value)

if result[0] < result[4]["5%"]:
    print ("Time Series is Stationary")
   
else:
    print ("Time Series is Non-Stationary")

result = kpss(diff_vals['sun_diff'].values)
print('KPSS Statistics: ',result[0])
print('p-value: ', result[1])
print('Critical Values:')
for key, value in result[3].items():
    print(key, value)

if result[0] > result[3]["5%"]:
    print ("Time Series is not Stationary")
else:
    print ("Time Series is Stationary")

"""Now the results of both KPSS and ADF tests show that the data is stationary.

# **Modeling the time-series**

1.   ARIMA
2.   Seasonal ARIMA
3.   Deep Learning Approaches-
"""

#plotting lag plots
plt.figure(figsize=(12,8))
lag_plot(data.SUNACTIVITY)
plt.title('Original')
pyplot.show()
plt.figure(figsize=(12,8))
plt.title('Differenced')
lag_plot(diff_vals.sun_diff)

"""THe lag plot of the original time series shows some trend. This disappears for the differenced time series, as shown in the figures above.

**ACF and PACF of the original dataset**
"""

plot_acf(data.SUNACTIVITY,lags=50,use_vlines=True)
pyplot.show()

plot_pacf(data.SUNACTIVITY,lags=50,use_vlines=True)
pyplot.show()

"""**ACF and PACF of the differenced dataset**"""

plot_acf(diff_vals.sun_diff,lags=50,use_vlines=True,alpha=0.05)
pyplot.show()
plot_pacf(diff_vals.sun_diff,lags=50,use_vlines=True)
pyplot.show()

"""**EStimating ARIMA paramenters(p,d,q) using GridSearch(with least MSE)**"""

def eval_arima_model(dataset,order):
  
  train_size = int(len(dataset)*0.65)
  train,test = dataset[0:train_size],dataset[train_size:]
  history = [x for x in train]
  preds = []
  for b in range(len(test)):
    model = ARIMA(history,order = order)
    model_fit = model.fit()
    yhat = model_fit.forecast()[0]
    preds.append(yhat)
    history.append(test[b])

  mse = mean_squared_error(test,preds)
  
  return mse



def eval_models(dataset,pvals,dvals,qvals):
  dataset = dataset.astype("float32")
  best_mse,best_params = float("inf"),None
  for p in pvals:
    for d in dvals:
      for q in qvals:
        order = (p,d,q)
        try:
          mse = eval_arima_model(dataset,order)
          if mse < best_mse:
            best_mse,best_params = mse,order
          
          print('ARIMA%s MSE=%.3f' % (order,mse))
        except:
          continue
  print('BEST ARIMA%s MSE=%.3f' % (best_params,best_mse))

pvals = range(2,6)
dvals = range(0, 3)
qvals = range(2,10)
warnings.filterwarnings("ignore")
eval_models(diff_vals['sun_diff'].values, pvals, dvals,qvals)

pvals = range(6,10)
dvals = range(0, 3)
qvals = range(2,10)
warnings.filterwarnings("ignore")
eval_models(diff_vals['sun_diff'].values, pvals, dvals,qvals)

"""From the above steps, BEST ARIMA parameters obtained are : p = 9, d = 2, q = 3.

**Visualising seasonal decomposition**
"""

result = seasonal_decompose(data['SUNACTIVITY'], model='additive', freq=12)

fig = plt.figure()  
fig = result.plot()  
fig.set_size_inches(10,7)

data.isnull().sum()

"""1. **ARIMA(9,0,0)**"""

X = data['SUNACTIVITY'].values
size = int(len(X) * 0.66)
train, test = X[0:size], X[size:len(X)]
history = [x for x in train]
preds = []

for t in range(len(test)):
	model = ARIMA(history, order=(9,0,0))
	model_fit = model.fit()
	output = model_fit.forecast()
	yhat = output[0]
	preds.append(yhat)
	obs = test[t]
	history.append(obs)


rmse = sqrt(mean_squared_error(test, preds))
mse = mean_squared_error(test,preds)
r2score = r2_score(test,preds)
print('Test MSE: %.3f' % mse)
print('Test RMSE: %.3f' % rmse)
print('Test R2SCORE: %.3f' % r2score)
fig = plt.figure(figsize=(10, 6))
ax = fig.subplots()
pyplot.plot(test,label = 'actual')
pyplot.plot(preds, color='red',label = 'predicted')


   
ax.legend(loc='lower right')
plt.title('AR')

pyplot.show()

"""2. **ARIMA(0,0,3)**"""

X = data['SUNACTIVITY'].values
size = int(len(X) * 0.66)
train, test = X[0:size], X[size:len(X)]
history = [x for x in train]
preds = []

for t in range(len(test)):
	model = ARIMA(history, order=(0,0,3))
	model_fit = model.fit()
	output = model_fit.forecast()
	yhat = output[0]
	preds.append(yhat)
	obs = test[t]
	history.append(obs)


rmse = sqrt(mean_squared_error(test, preds))
mse = mean_squared_error(test,preds)
r2score = r2_score(test,preds)
print('Test MSE: %.3f' % mse)
print('Test RMSE: %.3f' % rmse)
print('Test R2SCORE: %.3f' % r2score)
fig = plt.figure(figsize=(10, 6))
ax = fig.subplots()
pyplot.plot(test,label = 'actual')
pyplot.plot(preds, color='red',label = 'predicted')


   
ax.legend(loc='lower right')
plt.title('MA')

pyplot.show()

"""3. **ARIMA(9,0,3)**"""

X = data['SUNACTIVITY'].values
size = int(len(X) * 0.66)
train, test = X[0:size], X[size:len(X)]
history = [x for x in train]
preds = []

for t in range(len(test)):
	model = ARIMA(history, order=(9,0,3))
	model_fit = model.fit()
	output = model_fit.forecast()
	yhat = output[0]
	preds.append(yhat)
	obs = test[t]
	history.append(obs)


rmse = sqrt(mean_squared_error(test, preds))
mse = mean_squared_error(test,preds)
r2score = r2_score(test,preds)
print('Test MSE: %.3f' % mse)
print('Test RMSE: %.3f' % rmse)
print('Test R2SCORE: %.3f' % r2score)
fig = plt.figure(figsize=(10, 6))
ax = fig.subplots()
pyplot.plot(test,label = 'actual')
pyplot.plot(preds, color='red',label = 'predicted')


   
ax.legend(loc='lower right')
plt.title('ARMA')

pyplot.show()

"""4. **ARIMA(9,2,3)**"""

X = data['SUNACTIVITY'].values
size = int(len(X) * 0.66)
train, test = X[0:size], X[size:len(X)]
history = [x for x in train]
preds = []

for t in range(len(test)):
	model = ARIMA(history, order=(9,2,3))
	model_fit = model.fit()
	output = model_fit.forecast()
	yhat = output[0]
	preds.append(yhat)
	obs = test[t]
	history.append(obs)


rmse = sqrt(mean_squared_error(test, preds))
mse = mean_squared_error(test,preds)
r2score = r2_score(test,preds)
print('Test MSE: %.3f' % mse)
print('Test RMSE: %.3f' % rmse)
print('Test R2SCORE: %.3f' % r2score)
fig = plt.figure(figsize=(10, 6))
ax = fig.subplots()
pyplot.plot(test,label = 'actual')
pyplot.plot(preds, color='red',label = 'predicted')


   
ax.legend(loc='lower right')
plt.title('ARIMA')

pyplot.show()

msevals = pd.DataFrame(columns=['Models','Test MSE','Test R2score'])
msevals['Models'] = ['AR','MA','ARMA','ARIMA']
msevals['Test MSE'] = [293.047,490.999,324.704,309.941]
msevals['Test R2score'] = [0.869,0.78,0.854,0.865]
msevals

"""**Seasonal ARIMA**"""

from itertools import product
from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error
from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error

from scipy.optimize import minimize
import statsmodels.tsa.api as smt
import statsmodels.api as sm

from tqdm import tqdm_notebook

ps = range(0, 5)
d = 1
qs = range(0, 5)
Ps = range(0, 5)
D = 1
Qs = range(0, 5)
s = 5

#Create a list with all possible combinations of parameters
parameters = product(ps, qs, Ps, Qs)
parameters_list = list(parameters)
len(parameters_list)

# Train many SARIMA models to find the best set of parameters
def optimize_SARIMA(parameters_list, d, D, s):

    results = []
    best_aic = float('inf')
    
    for param in tqdm_notebook(parameters_list):
        try: model = sm.tsa.statespace.SARIMAX(diff_vals.sun_diff, order=(param[0], d, param[1]),
                                               seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)
        except:
            continue
            
        aic = model.aic
        
        #Save best model, AIC and parameters
        if aic < best_aic:
            best_model = model
            best_aic = aic
            best_param = param
        results.append([param, model.aic])
        
    result_table = pd.DataFrame(results,columns=['params','aic'])

    #Sort in ascending order, lower AIC is better
    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)
    
    return result_table

result_table = optimize_SARIMA(parameters_list, d, D, s)
print(result_table)

#Set parameters that give the lowest AIC (Akaike Information Criteria)

p, q, P, Q = result_table.params[0]

best_model = sm.tsa.statespace.SARIMAX(diff_vals.sun_diff, order=(p, d, q),
                                       seasonal_order=(P, D, Q, s)).fit(disp=-1)

print(best_model.summary())

"""# **Forecasting using Deep Learning Approaches**


1.   Using DNN
2.   Using RNN

3.  
Using LSTM

*1. Using DNN*
"""

df_arr= diff_vals['sun_diff'].values
train_size = int(len(df_arr)*0.66)
test_size = len(df_arr)-(train_size)
test_size

#reshaping dataset
def conv_to_matrix(array, back):
 X, Y =[], []
 for i in range(len(array)-back):
  d=i+back  
  X.append(array[i:d,0])
  Y.append(array[d,0])
 return np.array(X), np.array(Y)

train_size = int(len(diff_vals['sun_diff'].values)*0.9) #splitting dataset into train and test

df_arr= diff_vals['sun_diff'].values
df_arr = np.reshape(df_arr, (-1, 1))
test_size = len(df_arr)-(train_size)
train, test = df_arr[0:train_size,:], df_arr[train_size:len(df_arr),:]
back = 10
trainX, trainY = conv_to_matrix(train, back)
testX, testY = conv_to_matrix(test, back)

#making a sequential model
def model_dnn(back):
    model=Sequential()
    model.add(Dense(units=32, input_dim=back, activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error',  optimizer='adam',metrics = ['mse', 'mae'])
    return model

#training the model
back=10
model=model_dnn(back)
history=model.fit(trainX,trainY, epochs=500, batch_size=30, verbose=1, validation_data=(testX,testY),callbacks=[EarlyStopping(monitor='val_loss', patience=10)],shuffle=False)

def model_loss(history):
    plt.figure(figsize=(12,6))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Test Loss')
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epochs')
    plt.legend(loc='upper right')
    plt.show();

# Commented out IPython magic to ensure Python compatibility.

train_score = model.evaluate(trainX, trainY, verbose=0)
print('Train Root Mean Squared Error(RMSE): %.2f; Train Mean Absolute Error(MAE) : %.2f ' 
# % (np.sqrt(train_score[1]), train_score[2]))
test_score = model.evaluate(testX, testY, verbose=0)
print('Test Root Mean Squared Error(RMSE): %.2f; Test Mean Absolute Error(MAE) : %.2f ' 
# % (np.sqrt(test_score[1]), test_score[2]))
model_loss(history)

def prediction_plot(trainY, train_predict):
      len_prediction=[x for x in range(len(testY))]
      plt.figure(figsize=(12,6))
      plt.plot(len_prediction, testY[:], marker='.', label="actual")
      plt.plot(len_prediction, test_predict[:], 'r', label="prediction")
      plt.tight_layout()
      
      plt.title('SUNSPOTS')
      plt.ylabel('Sunactivity', size=15)
      plt.xlabel('Years', size=15)
      plt.legend(fontsize=15)
      plt.show();

train_predict = model.predict(trainX)
test_predict = model.predict(testX)

prediction_plot(testX,test_predict)

"""2. *Using RNN*"""

#defining model with 2 layers

def model_rnn(look_back):
  model=Sequential()
  model.add(SimpleRNN(units=32, input_shape=(1,look_back), activation="relu"))
  model.add(Dense(8, activation='relu'))
  model.add(Dense(1))
  model.compile(loss='mean_squared_error',  optimizer='adam',metrics = ['mse', 'mae'])
  return model

model=model_rnn(look_back)
history=model.fit(trainX,trainY, epochs=100, batch_size=30, verbose=1, validation_data=(testX,testY),callbacks=[EarlyStopping(monitor='val_loss', patience=10)],shuffle=False)



# Commented out IPython magic to ensure Python compatibility.

train_predict = model.predict(trainX)
test_predict = model.predict(testX)
print('Train Root Mean Squared Error(RMSE): %.2f; Train Mean Absolute Error(MAE) : %.2f '
#       % (np.sqrt(mean_squared_error(trainY, train_predict[:,0])), mean_absolute_error(trainY, train_predict[:,0])))
print('Test Root Mean Squared Error(RMSE): %.2f; Test Mean Absolute Error(MAE) : %.2f ' 
#       % (np.sqrt(mean_squared_error(testY, test_predict[:,0])), mean_absolute_error(testY, test_predict[:,0])))
model_loss(history)

def prediction_plot(testY, test_predict,look_back):
    len_prediction=[x for x in range(len(testY))]
    plt.figure(figsize=(12,6))
    plt.plot(len_prediction, testY[:], marker='.', label="actual")
    plt.plot(len_prediction, test_predict[:], 'r', label="prediction")
    plt.tight_layout()
    
    plt.ylabel('Sunactivity', size=15)
    plt.xlabel('Years', size=15)
    plt.legend(fontsize=15)
    plt.title('SUNSPOTS')
    plt.show()

prediction_plot(testY, test_predict,look_back)

"""3. *Using LSTM*"""

from sklearn.preprocessing import MinMaxScaler
#create numpy.ndarray
df_arr= diff_vals['sun_diff'].values
#diff_vals['sun_diff'].values = df_arr.astype('float32')
df_arr = np.reshape(df_arr, (-1, 1)) #LTSM requires more input features compared to RNN or DNN
scaler = MinMaxScaler(feature_range=(0, 1))
df_arr = scaler.fit_transform(df_arr)

train_size = int(len(df_arr)*0.66)
test_size = len(df_arr)-(train_size)
test_size
test_size = len(df_arr)-(train_size)
train, test = df_arr[0:train_size,:], df_arr[train_size:len(df_arr),:]
look_back = 10
trainX, trainY = conv_to_matrix(train, look_back)
testX, testY = conv_to_matrix(test, look_back)

trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

def model_lstm(look_back):
    model=Sequential()
    model.add(LSTM(100, input_shape=(1, look_back), activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error',  optimizer='adam',metrics = ['mse', 'mae'])
    return model

model=model_lstm(look_back)
history = model.fit(trainX, trainY, epochs=100, batch_size=30, validation_data=(testX, testY), callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)

train_predict = model.predict(trainX)
test_predict = model.predict(testX)

def model_loss(history):
    plt.figure(figsize=(12,6))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Test Loss')
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epochs')
    plt.legend(loc='upper right')
    plt.show();

# Commented out IPython magic to ensure Python compatibility.

train_score = model.evaluate(trainX, trainY, verbose=0)
print('Train Root Mean Squared Error(RMSE): %.2f; Train Mean Absolute Error(MAE) : %.2f ' 
# % (np.sqrt(train_score[1]), train_score[2]))
test_score = model.evaluate(testX, testY, verbose=0)
print('Test Root Mean Squared Error(RMSE): %.2f; Test Mean Absolute Error(MAE) : %.2f ' 
# % (np.sqrt(test_score[1]), test_score[2]))
model_loss(history)

def prediction_plot(testY, test_predict,look_back):
    plt.figure(figsize=(12,6))
    len_prediction=[x for x in range(len(testY))]
    plt.plot(len_prediction, testY[:], marker='.', label="actual")
    plt.plot(len_prediction, test_predict[:], 'r', label="prediction")
    plt.tight_layout()

    plt.ylabel('SUNACTIVITY', size=15)
    plt.xlabel('Years', size=15)
    plt.legend(fontsize=15)
    plt.title('Sunspots')
    plt.show()

prediction_plot(testY, test_predict,look_back)

"""# **Clustering**

1. **K-Means**
"""

X = data.iloc[:,[0,1]].values

# Using the elbow method to find the optimal number of clusters
from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X)
    # inertia method returns wcss for that model
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10,5))
sns.lineplot(range(1, 11), wcss,marker='o',color='red')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X)

# Visualising the clusters
plt.figure(figsize=(15,7))
sns.scatterplot(X[y_kmeans == 0,0], X[y_kmeans == 0,1], color = 'yellow', label = 'Cluster 1',s=50)
sns.scatterplot(X[y_kmeans == 1,0], X[y_kmeans == 1,1], color = 'blue', label = 'Cluster 2',s=50)
sns.scatterplot(X[y_kmeans == 2,0], X[y_kmeans == 2,1], color = 'green', label = 'Cluster 3',s=50)
sns.scatterplot(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color = 'red', 
                label = 'Centroids',s=300,marker=',')
plt.grid(False)
plt.title('SUNACTIVITY')
plt.xlabel('YEARS')
plt.ylabel('Values')
plt.legend()
plt.show()



"""2. **AgglomerativeClustering**"""

plt.figure(figsize=(15,7))
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))

# create clusters
hc = AgglomerativeClustering(n_clusters=3, affinity = 'euclidean', linkage = 'ward')
# save clusters for chart
y_means = hc.fit_predict(X)

plt.figure(figsize=(15,7))
sns.scatterplot(X[y_kmeans == 0,0], X[y_kmeans == 0,1], color = 'pink', label = 'Cluster 1',s=50)
sns.scatterplot(X[y_kmeans == 1,0], X[y_kmeans == 1,1], color = 'cyan', label = 'Cluster 2',s=50)
sns.scatterplot(X[y_kmeans == 2,0], X[y_kmeans == 2,1], color = 'red', label = 'Cluster 3',s=50)

sns.scatterplot(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color = 'red', 
                label = 'Centroids',s=300,marker=',')
plt.grid(False)
plt.title('SUNACTIVITY')
plt.xlabel('YEARS')
plt.ylabel('Values')
plt.legend()
plt.show()



"""# **Outlier Detection**

1. **InterQuartile Range Method**
"""

def out_iqr(df , column):
    global lower,upper
    q25, q75 = np.quantile(df[column], 0.25), np.quantile(df[column], 0.75)
    # calculate the IQR
    iqr = q75 - q25
    # calculate the outlier cutoff
    cut_off = iqr * 1.5
    # calculate the lower and upper bound value
    lower, upper = q25 - cut_off, q75 + cut_off
    print('The IQR is',iqr)
    print('The lower bound value is', lower)
    print('The upper bound value is', upper)
    # Calculate the number of records below and above lower and above bound value respectively
    df1 = df[df[column] > upper]
    df2 = df[df[column] < lower]
    return print('Total number of outliers are', df1.shape[0]+ df2.shape[0])

out_iqr(data,'SUNACTIVITY')

plt.figure(figsize = (10,6))
sns.distplot(data.SUNACTIVITY, kde=False)
plt.axvspan(xmin = lower,xmax= data.SUNACTIVITY.min(),alpha=0.5, color='red')
plt.axvspan(xmin = upper,xmax= data.SUNACTIVITY.max(),alpha=0.5, color='red')

"""2. **Standard Deviation Method**"""

plt.figure(figsize = (10,5))
sns.distplot(data['SUNACTIVITY'])

"""SInce the data is Gaussian, STD DEV method can be applied to check for outliers."""

def out_std(df, column):
    global lower,upper
    # calculate the mean and standard deviation of the data frame
    data_mean, data_std = df[column].mean(), df[column].std()
    # calculate the cutoff value
    cut_off = data_std * 3
    # calculate the lower and upper bound value
    lower, upper = data_mean - cut_off, data_mean + cut_off
    print('The lower bound value is', lower)
    print('The upper bound value is', upper)
    # Calculate the number of records below and above lower and above bound value respectively
    df1 = df[df[column] > upper]
    df2 = df[df[column] < lower]
    return print('Total number of outliers are', df1.shape[0]+ df2.shape[0])

out_std(data,'SUNACTIVITY')

plt.figure(figsize = (10,5))
sns.distplot(data.SUNACTIVITY, kde=False)
plt.axvspan(xmin = lower,xmax= data.SUNACTIVITY.min(),alpha=0.5, color='red')
plt.axvspan(xmin = upper,xmax= data.SUNACTIVITY.max(),alpha=0.5, color='red')

"""3. **Z-Score Method**"""

def out_zscore(data):
    global outliers,zscore
    outliers = []
    zscore = []
    threshold = 3
    mean = np.mean(data)
    std = np.std(data)
    for i in data:
        z_score= (i - mean)/std 
        zscore.append(z_score)
        if np.abs(z_score) > threshold:
            outliers.append(i)
    return print("Total number of outliers are",len(outliers))

out_zscore(data.SUNACTIVITY)

plt.figure(figsize = (10,5))
sns.distplot(zscore)
plt.axvspan(xmin = 3 ,xmax= max(zscore),alpha=0.5, color='red')

